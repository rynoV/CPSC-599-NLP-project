#+title: CPSC 599 NLP Project Experimentation
#+author: 30093813
#+property: header-args  :tangle yes :comments link :session t :results output :exports both

#+begin_src python
from util import change_extension, sliding_window
from collections import defaultdict
import nltk
import spacy
from spacy.tokens import Token, DocBin
import en_core_web_sm
import re
import json
import os
import glob
import components
#+end_src

#+RESULTS:
* Evaluating trained model
#+begin_src python
trained = spacy.load("./packages/en_pipeline-0.0.7/en_pipeline/en_pipeline-0.0.7/")
print(trained.pipe_names)
print(trained.component_names)
trained.analyze_pipes(pretty=True)
#+end_src

#+RESULTS:
#+begin_example
['transformer', 'spancat']
['transformer', 'spancat']
[1m
============================= Pipeline Overview =============================[0m

#   Component     Assigns          Requires   Scores       Retokenizes
-   -----------   --------------   --------   ----------   -----------
0   transformer   doc._.trf_data                           False

1   spancat       doc.spans                   spans_sc_f   False
                                              spans_sc_p
                                              spans_sc_r

[38;5;2mâœ” No problems found.[0m
#+end_example

#+begin_src python
def show_spans(doc):
    print(doc)
    print(*((s, s.label_, s.start, s.end) for s in doc.spans['sc']))
#+end_src

#+RESULTS:

#+begin_src python
data_dir = 'data/html/processed/'
pattern = os.path.join(data_dir, '**', '*' + '.txt')
all_txt = glob.glob(pattern, recursive=True)
texts = []
for txt in all_txt:
    with open(txt, 'r') as f:
        texts.append(f.read())

# TODO: update to apply same preprocessing to these texts before making predictions
print(all_txt.index('data/html/processed/unsupervised_learning.txt'))
for doc in trained.pipe(texts[:10]):
    l = len(doc.spans['sc'])
    if l > 0:
        show_spans(doc)
#+end_src

#+RESULTS:
#+begin_example
10
{'sc': [WRAPEND]}
[(WRAPEND, 'https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend')]
{'sc': [WRAPEND_]}
[(WRAPEND_, 'modules/generated/sklearn.ensemble.gradientboostingregressor.html#sklearn.ensemble.gradientboostingregressor')]
{'sc': [fit, WRAPEND]}
[(fit, 'glossary.html#term-fit'), (WRAPEND, 'https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend')]
{'sc': [WRAPEND_]}
[(WRAPEND_, 'glossary.html#term-classes_')]
{'sc': [WRAPEND]}
[(WRAPEND, 'https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend')]
{'sc': [WRAPEND, _]}
[(WRAPEND, 'https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend'), (_, 'glossary.html#term-decision_function')]
{'sc': [Thomas Fan, Andreas MÃ¼ller, Adrin Jalali, SLEP _]}
[(Thomas Fan, 'https://github.com/thomasjpfan'), (Andreas MÃ¼ller, 'https://amueller.github.io/'), (Adrin Jalali, 'https://github.com/adrinjalali'), (SLEP _, 'glossary.html#term-predict_proba')]
#+end_example

#+begin_src python
exs = ['One needs to fit the model to the data.', 'Refer to the glossary'
       , 'This was provided by Thomas Fan'
       , 'To provide a good fit the model needs to be trained.'
       , 'A pipeline can be used to improve the model by using cross validation.'
       , 'This is referred to as a warm start'
       , 'In classification the model is trained to predict a number of classes'
       , 'Classification algorithms usually also offer a way to quantify certainty of a prediction, either using decision_function or predict_proba'
       , 'All built-in estimators also have a set_params method, which sets data-independent parameters (overriding previous parameter values passed to __init__).'
       , 'y might be ignored in the case of unsupervised learning. However, to make it possible to use the estimator as part of a pipeline that can mix both supervised and unsupervised transformers, even unsupervised estimators need to accept a y=None keyword argument in the second position that is just ignored by the estimator.'
       , 'When fit is called, any previous call to fit should be ignored. In general, calling estimator.fit(X1) and then estimator.fit(X2) should be the same as only calling estimator.fit(X2). However, this may not be true in practice when fit depends on some random process, see random_state. Another exception to this rule is when the hyper-parameter warm_start is set to True for estimators that support it.'
       , 'Attributes that have been estimated from the data must always have a name ending with trailing underscore, for example the coefficients of some regression estimator would be stored in a coef_ attribute after fit has been called.'
       , 'Often, the subestimator has a name (as e.g. named steps in a Pipeline object), in which case the key should become <name>__C, <name>__class_weight, etc.'
       , 'Often, the subestimator has a name (as e.g. named steps in Pipeline objects), in which case the key should become <name>__C, <name>__class_weight, etc.'
       , 'For an estimator to be usable together with pipeline.Pipeline in any but the last step, it needs to provide a fit or fit_transform function. To be able to evaluate the pipeline on any data but the training set, it also needs to provide a transform function. There are no special requirements for the last step in a pipeline, except that it has a fit function. All fit and fit_transform functions must take arguments X, y, even if y is not used. Similarly, for score to be usable, the last step of the pipeline needs to have a score function that accepts an optional y.'
       , 'In a Pipeline object you can use GridSearchCV or RandomForestClassifier'
       , 'Whether you are proposing an estimator for inclusion in scikit-learn, developing a separate package compatible with scikit-learn, or implementing custom components for your own projects, this chapter details how to develop objects that safely interact with scikit-learn Pipelines and model selection tools.'
       , 'The base object, implements a fit method to learn from data'
       , 'Elements of the scikit-learn API are described more definitively in the Glossary of Common Terms and API Elements.']
for doc in trained.pipe(exs):
    show_spans(doc)
    print()
#+end_src

#+RESULTS:
#+begin_example
One needs to fit the model to the data.


Refer to the glossary


This was provided by Thomas Fan
(Thomas Fan, 'https://github.com/thomasjpfan', 4, 6)

To provide a good fit the model needs to be trained.


A pipeline can be used to improve the model by using cross validation.


This is referred to as a warm start


In classification the model is trained to predict a number of classes


Classification algorithms usually also offer a way to quantify certainty of a prediction, either using decision_function or predict_proba
(decision_function, 'glossary.html#term-decision_function', 16, 17) (predict_proba, 'glossary.html#term-predict_proba', 18, 19)

All built-in estimators also have a set_params method, which sets data-independent parameters (overriding previous parameter values passed to __init__).
(_, 'sklearn.metrics.r2_score.html#sklearn.metrics.r2_score', 27, 28)

y might be ignored in the case of unsupervised learning. However, to make it possible to use the estimator as part of a pipeline that can mix both supervised and unsupervised transformers, even unsupervised estimators need to accept a y=None keyword argument in the second position that is just ignored by the estimator.
(pipeline, 'modules/generated/sklearn.pipeline.pipeline.html#sklearn.pipeline.pipeline', 25, 26)

When fit is called, any previous call to fit should be ignored. In general, calling estimator.fit(X1) and then estimator.fit(X2) should be the same as only calling estimator.fit(X2). However, this may not be true in practice when fit depends on some random process, see random_state. Another exception to this rule is when the hyper-parameter warm_start is set to True for estimators that support it.
(warm_start, 'glossary.html#term-random_state', 65, 66)

Attributes that have been estimated from the data must always have a name ending with trailing underscore, for example the coefficients of some regression estimator would be stored in a coef_ attribute after fit has been called.
(fit, 'glossary.html#term-fit', 35, 36)

Often, the subestimator has a name (as e.g. named steps in a Pipeline object), in which case the key should become <name>__C, <name>__class_weight, etc.
(Pipeline, 'modules/generated/sklearn.pipeline.pipeline.html#sklearn.pipeline.pipeline', 14, 15)

Often, the subestimator has a name (as e.g. named steps in Pipeline objects), in which case the key should become <name>__C, <name>__class_weight, etc.
(Pipeline, 'sklearn.pipeline.pipeline.html#sklearn.pipeline.pipeline', 13, 14)

For an estimator to be usable together with pipeline.Pipeline in any but the last step, it needs to provide a fit or fit_transform function. To be able to evaluate the pipeline on any data but the training set, it also needs to provide a transform function. There are no special requirements for the last step in a pipeline, except that it has a fit function. All fit and fit_transform functions must take arguments X, y, even if y is not used. Similarly, for score to be usable, the last step of the pipeline needs to have a score function that accepts an optional y.


In a Pipeline object you can use GridSearchCV or RandomForestClassifier
(GridSearchCV, 'modules/generated/sklearn.model_selection.gridsearchcv.html#sklearn.model_selection.gridsearchcv', 7, 8) (RandomForestClassifier, 'modules/generated/sklearn.ensemble.randomforestclassifier.html#sklearn.ensemble.randomforestclassifier', 9, 10)

Whether you are proposing an estimator for inclusion in scikit-learn, developing a separate package compatible with scikit-learn, or implementing custom components for your own projects, this chapter details how to develop objects that safely interact with scikit-learn Pipelines and model selection tools.


The base object, implements a fit method to learn from data
(fit, 'glossary.html#term-fit', 6, 7)

Elements of the scikit-learn API are described more definitively in the Glossary of Common Terms and API Elements.
#+end_example

#+begin_src python
db = DocBin().from_disk('test.spacy')
docs = list(db.get_docs(trained.vocab))
common_labels = ['https://sphinx-gallery.github.io']
desired_labels = [
    # 'glossary.html#term-fit',
    # 'glossary.html#term-random_state',
    # 'glossary.html#term-n_jobs',
    # 'https://github.com/scikit-learn/scikit-learn/blob/449940985/sklearn/base.py#l153',
    'sklearn.pipeline.pipeline.html#sklearn.pipeline.pipeline'
]
c = 0
for doc, orig_spans in zip(trained.pipe(doc.text for doc in docs), (doc.spans['sc'] for doc in docs)):
    spans = doc.spans['sc']
    # if len(spans) > 0 and any(s.label_ not in common_labels for s in spans):
    # if len(spans) > 0 and any(s.label_ in desired_labels for s in spans):
    links = set(s.label_ for s in spans)
    orig_links = set(s.label_ for s in orig_spans)
    if orig_links != links:
        c += 1
        print(*((s, s.label_, s.start, s.end) for s in orig_spans))
        show_spans(doc)
        if c > 5:
            break
#+end_src

#+RESULTS:
#+begin_example
(Lars Buitinck, 'https://github.com/larsmans', 31, 33) (Andreas MÃ¼ller, 'https://amueller.github.io/', 38, 40) (Gael Varoquaux, 'https://gael-varoquaux.info', 45, 47) (Gilles Louppe, 'http://www.montefiore.ulg.ac.be/~glouppe/', 85, 87) (Mathieu Blondel, 'http://www.mblondel.org', 92, 94)


-   Other small improvements to tests and documentation.

People Â¶

List of contributors for release 0.13.1 by number of commits.
-   16  Lars Buitinck

-   12  Andreas MÃ¼ller

-   8  Gael Varoquaux

-   5 Robert Marchman

-   3  Peter Prettenhofer

-   2 Hrishikesh Huilgolkar

-   1 Bastiaan van den Berg

-   1 Diego Molla

-   1  Gilles Louppe

-   1  Mathieu Blondel

-   1  Nelle Varoquaux

-   1 Rafael Cunha de Almeida

-   1 Rolando Espinoza La fuente

-   1  Vlad Niculae

-   1  Yaroslav Halchenko

Version 0.13 Â¶

January 21, 2013

New Estimator Classes Â¶

-    dummy.
(Gilles Louppe, 'http://www.montefiore.ulg.ac.be/~glouppe/', 85, 87) (Mathieu Blondel, 'http://www.mblondel.org', 92, 94)
(Mathieu Blondel, 'http://www.mblondel.org', 139, 141)

-   16  Lars Buitinck

-   12  Andreas MÃ¼ller

-   8  Gael Varoquaux

-   5 Robert Marchman

-   3  Peter Prettenhofer

-   2 Hrishikesh Huilgolkar

-   1 Bastiaan van den Berg

-   1 Diego Molla

-   1  Gilles Louppe

-   1  Mathieu Blondel

-   1  Nelle Varoquaux

-   1 Rafael Cunha de Almeida

-   1 Rolando Espinoza La fuente

-   1  Vlad Niculae

-   1  Yaroslav Halchenko

Version 0.13 Â¶

January 21, 2013

New Estimator Classes Â¶

-    dummy.DummyClassifier  and  dummy.DummyRegressor , two data-independent predictors by  Mathieu Blondel .
(Mathieu Blondel, 'http://www.mblondel.org', 139, 141) (Gilles Louppe, 'http://www.montefiore.ulg.ac.be/~glouppe/', 59, 61) (Mathieu Blondel, 'http://www.mblondel.org', 66, 68) (dummy.DummyRegressor, 'modules/generated/sklearn.ensemble.gradientboostingregressor.html#sklearn.ensemble.gradientboostingregressor', 128, 131)
(Lars Buitinck, 'https://github.com/larsmans', 56, 58)
SelectPercentile  are more numerically stable since they use scores, rather than p-values, to rank results. This means that they might sometimes select different features than they did previously.

-   Ridge regression and ridge classification fitting with sparse_cg solver no longer has quadratic memory complexity, by  Lars Buitinck  and  Fabian Pedregosa .
(Lars Buitinck, 'https://github.com/larsmans', 56, 58) (Ridge, 'modules/generated/sklearn.linear_model.ridge.html#sklearn.linear_model.ridge', 38, 39) (ridge, 'modules/generated/sklearn.linear_model.ridge.html#sklearn.linear_model.ridge', 41, 42)
(Mathieu Blondel, 'http://www.mblondel.org', 62, 64)
This means that they might sometimes select different features than they did previously.

-   Ridge regression and ridge classification fitting with sparse_cg solver no longer has quadratic memory complexity, by  Lars Buitinck  and  Fabian Pedregosa .

-   Ridge regression and ridge classification now support a new fast solver called lsqr, by  Mathieu Blondel .
(Mathieu Blondel, 'http://www.mblondel.org', 62, 64) (ridge, 'modules/generated/sklearn.linear_model.ridge.html#sklearn.linear_model.ridge', 49, 50) (Ridge, 'modules/generated/sklearn.linear_model.ridge.html#sklearn.linear_model.ridge', 46, 47)
(Andreas MÃ¼ller, 'https://amueller.github.io/', 43, 45) (Arnaud Joly, 'http://www.ajoly.org', 50, 52) (Gael Varoquaux, 'https://gael-varoquaux.info', 64, 66) (Mathieu Blondel, 'http://www.mblondel.org', 71, 73) (Lars Buitinck, 'https://github.com/larsmans', 78, 80) (Olivier Grisel, 'https://twitter.com/ogrisel', 91, 93) (Gilles Louppe, 'http://www.montefiore.ulg.ac.be/~glouppe/', 105, 107) (Alexandre Gramfort, 'http://alexandre.gramfort.net', 119, 121)


-   In the Naive Bayes classifiers, the class_prior parameter was moved from fit to __init__.

People Â¶

List of contributors for release 0.13 by number of commits.

  -   364  Andreas MÃ¼ller

  -   143  Arnaud Joly

  -   137  Peter Prettenhofer

  -   131  Gael Varoquaux

  -   117  Mathieu Blondel

  -   108  Lars Buitinck

  -   106 Wei Li

  -   101  Olivier Grisel

  -   65  Vlad Niculae

  -   54  Gilles Louppe

  -   40  Jaques Grobler

  -   38  Alexandre Gramfort

  -   30  Rob Zinkov

  -   19 Aymeric Masurelle

  -   18 Andrew Winterman

  -   17  Fabian Pedregosa

  -   17 Nelle Varoquaux

  -   16  Christian Osendorfer

  -   14  Daniel Nouri

  -   13  Virgile Fritsch

  -   13 syhw

  -   12  Satrajit Ghosh

  -   10 Corey Lynch

  -   10 Kyle Beauchamp

  -   9 Brian Cheung

  -   9 Immanuel Bayer

  -   9 mr.
(Olivier Grisel, 'https://twitter.com/ogrisel', 91, 93) (Mathieu Blondel, 'http://www.mblondel.org', 71, 73) (Alexandre Gramfort, 'http://alexandre.gramfort.net', 119, 121) (Gael Varoquaux, 'https://gael-varoquaux.info', 64, 66) (Gilles Louppe, 'http://www.montefiore.ulg.ac.be/~glouppe/', 105, 107) (Arnaud Joly, 'http://www.ajoly.org', 50, 52) (Andreas MÃ¼ller, 'https://amueller.github.io/', 43, 45)
(Thomas Fan, 'https://github.com/thomasjpfan', 288, 290)


-   Version 0.24.2
    -   Changelog
        -   sklearn.compose
        -   sklearn.cross_decomposition
        -   sklearn.decomposition
        -   sklearn.ensemble
        -   feature_extraction
        -   sklearn.gaussian_process
        -   sklearn.linear_model
        -   sklearn.metrics
        -   sklearn.model_selection
        -   sklearn.multioutput
        -   sklearn.preprocessing
        -   sklearn.semi_supervised
        -   sklearn.tree
        -   sklearn.utils
-   Version 0.24.1
    -   Packaging
    -   Changelog
        -   sklearn.metrics
        -   sklearn.semi_supervised
-   Version 0.24.0
    -   Legend for changelogs
    -   Changed models
    -   Changelog
        -   sklearn.base
        -   sklearn.calibration
        -   sklearn.cluster
        -   sklearn.compose
        -   sklearn.covariance
        -   sklearn.cross_decomposition
        -   sklearn.datasets
        -   sklearn.decomposition
        -   sklearn.discriminant_analysis
        -   sklearn.ensemble
        -   sklearn.exceptions
        -   sklearn.feature_extraction
        -   sklearn.feature_selection
        -   sklearn.gaussian_process
        -   sklearn.impute
        -   sklearn.inspection
        -   sklearn.isotonic
        -   sklearn.kernel_approximation
        -   sklearn.linear_model
        -   sklearn.manifold
        -   sklearn.metrics
        -   sklearn.model_selection
        -   sklearn.multiclass
        -   sklearn.multioutput
        -   sklearn.naive_bayes
        -   sklearn.neighbors
        -   sklearn.neural_network
        -   sklearn.pipeline
        -   sklearn.preprocessing
        -   sklearn.semi_supervised
        -   sklearn.svm
        -   sklearn.tree
        -   sklearn.utils
        -   Miscellaneous
    -   Code and Documentation Contributors

Version 0.24.2 Â¶

April 2021

Changelog Â¶

 sklearn.compose  Â¶

-   Fix compose.ColumnTransformer.get_feature_names does not call get_feature_names on transformers with an empty column selection.  #19579  by  Thomas Fan .
(Thomas Fan, 'https://github.com/thomasjpfan', 288, 290) (sklearn.preprocessing, 'classes.html#module-sklearn.preprocessing', 52, 53)
#+end_example

* Exploring preprocessing
#+begin_src python
data_dir = 'data/html/processed/'
pattern = os.path.join(data_dir, '**', '*' + '.txt')
all_txt = glob.glob(pattern, recursive=True)
print(len(all_txt))
#+end_src

#+RESULTS:
: 996

#+begin_src python
all_linkdata = []
all_links = []
for path in all_txt:
    with open(change_extension(path, '.linkdata.json'), 'r') as f:
        linkdata = json.load(f)
        all_linkdata.append(linkdata)
        for v in linkdata.values():
            all_links.append(v['link'])

all_links_set = set(all_links)
#+end_src

#+RESULTS:
** Lowercasing
We should check if the linkdata would benefit from normalization. In
particular, it may be the case that the links are case insensitive, or
are relative paths, in which case two different strings could be
considered the same link.

There's a few links that are the same but with different
capitalization:

#+begin_src python
link_forms = defaultdict(list)
for link in all_links_set:
    link_forms[link.lower()].append(link)
for links in link_forms.values():
    if len(links) > 1:
        print(links)
#+end_src

#+RESULTS:
#+begin_example
['https://github.com/GLevV', 'https://github.com/glevv']
['https://github.com/tomMoral', 'https://github.com/tommoral']
['#sklearn.covariance.OAS', '#sklearn.covariance.oas']
['https://github.com/Micky774', 'https://github.com/micky774']
['https://github.com/GuillemGSubies', 'https://github.com/guillemgsubies']
['https://github.com/NelleV', 'https://github.com/nellev']
['#sklearn.cluster.DBSCAN', '#sklearn.cluster.dbscan']
['#term-Y', '#term-y']
['#sklearn.decomposition.FastICA', '#sklearn.decomposition.fastica']
['https://github.com/NicolasHug', 'https://github.com/nicolashug']
['https://en.wikipedia.org/wiki/Mutual_information', 'https://en.wikipedia.org/wiki/Mutual_Information']
#+end_example

[[https://www.w3.org/TR/WD-html40-970708/htmlweb.html][Technically]] links are supposed to be case sensitive, but in this
dataset it appears we should treat the links as case insensitive. The
only questionable case is "term-y" vs "term-Y", however those both
lead to the same place ([[https://scikit-learn.org/stable/glossary.html#term-y][here]]). When making this a user-facing product,
it may be beneficial to have some settings around case sensitivity:
insensitive/sensitive by default, exceptions.

#+begin_src python
all_links_lower = [l.lower() for l in all_links]
all_links_lower_set = set(all_links_lower)
#+end_src

#+RESULTS:
** Link normalization

Next we'll explore normalization of the urls.

Initial exploration indicated that the only part of the url that could
be allowed to vary while still considering links to be equivalent was
the scheme; in particular "http" vs "https". The host (~netloc~) cannot
vary due to cases like "github.com/username" and
"twitter.com/username". The query/params cannot vary due to cases of
websites that show different pages based on the query, for example
youtube or mybinder. The fragment cannot vary because it may be used
to point to different places within the same page. So we'll count the
cases where the scheme differs across examples:

#+begin_src python
from urllib.parse import urlparse, urlunparse

link_paths = defaultdict(list)

for l in all_links_lower_set:
    parse = urlparse(l)
    p = parse.netloc + parse.path + parse.fragment + parse.query + parse.params
    if p != '':
        link_paths[p].append((l, parse))

change_scheme = {}
for links in link_paths.values():
    if len(links) > 1:
        print(links)
        has_scheme = lambda scheme, ls: filter(lambda l: l[1].scheme == scheme, ls)
        for httpsl, _ in has_scheme('https', links):
           for l, p in links:
               if p.scheme == 'http':
                   change_scheme[l] = httpsl
#+end_src

#+RESULTS:
: [('http://www.sciencedirect.com/science/article/pii/s016786550500303x', ParseResult(scheme='http', netloc='www.sciencedirect.com', path='/science/article/pii/s016786550500303x', params='', query='', fragment='')), ('https://www.sciencedirect.com/science/article/pii/s016786550500303x', ParseResult(scheme='https', netloc='www.sciencedirect.com', path='/science/article/pii/s016786550500303x', params='', query='', fragment=''))]
: [('http://scikit-learn.org/stable/modules/sgd.html', ParseResult(scheme='http', netloc='scikit-learn.org', path='/stable/modules/sgd.html', params='', query='', fragment='')), ('https://scikit-learn.org/stable/modules/sgd.html', ParseResult(scheme='https', netloc='scikit-learn.org', path='/stable/modules/sgd.html', params='', query='', fragment=''))]
: [('https://gael-varoquaux.info', ParseResult(scheme='https', netloc='gael-varoquaux.info', path='', params='', query='', fragment='')), ('http://gael-varoquaux.info', ParseResult(scheme='http', netloc='gael-varoquaux.info', path='', params='', query='', fragment=''))]
: [('http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf', ParseResult(scheme='http', netloc='jmlr.csail.mit.edu', path='/papers/volume2/crammer01a/crammer01a.pdf', params='', query='', fragment='')), ('https://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf', ParseResult(scheme='https', netloc='jmlr.csail.mit.edu', path='/papers/volume2/crammer01a/crammer01a.pdf', params='', query='', fragment=''))]
: [('http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf', ParseResult(scheme='http', netloc='jmlr.csail.mit.edu', path='/papers/volume11/vinh10a/vinh10a.pdf', params='', query='', fragment='')), ('https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf', ParseResult(scheme='https', netloc='jmlr.csail.mit.edu', path='/papers/volume11/vinh10a/vinh10a.pdf', params='', query='', fragment=''))]
: [('https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf', ParseResult(scheme='https', netloc='www.csie.ntu.edu.tw', path='/~cjlin/papers/libsvm.pdf', params='', query='', fragment='')), ('http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf', ParseResult(scheme='http', netloc='www.csie.ntu.edu.tw', path='/~cjlin/papers/libsvm.pdf', params='', query='', fragment=''))]

Not that many, but still worth taking into account. For this case,
changing the scheme may actually break the link, so we'll only change
the scheme to "https" for cases where there is already an "https"
variation of the link. We'll also do a roundtrip of parsing the url,
which should normalize alternate forms of the same link (in particular
'' and '#').

#+begin_src python
print(urlunparse(urlparse('#')))
#+end_src

#+RESULTS:

#+begin_src python
all_links_lower_norm = []
for l in all_links_lower:
    scheme_updated = change_scheme.get(l) or l
    all_links_lower_norm.append(urlunparse(urlparse(scheme_updated)))
print(len(change_scheme))
print(len(set(all_links_lower)))
all_links_lower_norm_set = set(all_links_lower_norm)
print(len(all_links_lower_norm_set))
#+end_src

#+RESULTS:
: 6
: 16905
: 16899

** Relative links
Finally we look at normalizing relative paths:

#+begin_src python
def norm_relative_link(link):
    if link.startswith('./') or link.startswith('../'):
        return re.sub(r'^(\.?\./)+', '', link)
    return link

link_tails = defaultdict(list)

for l in all_links_lower_norm_set:
    norm = norm_relative_link(l)
    link_tails[norm].append(l)

multis = [(norm, links) for norm, links in link_tails.items() if len(links) > 1]

print('Number of links with the same tail but different strings:', len(multis))
for norm, links in multis[:5]:
    if len(links) > 1:
        print(norm, links)
#+end_src

#+RESULTS:
: Number of links with the same tail but different strings: 646
: datasets/toy_dataset.html#iris-dataset ['../../datasets/toy_dataset.html#iris-dataset', '../datasets/toy_dataset.html#iris-dataset']
: auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py ['../../auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py', '../auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py']
: modules/clustering.html#clustering-evaluation ['../modules/clustering.html#clustering-evaluation', '../../modules/clustering.html#clustering-evaluation']
: modules/generated/sklearn.impute.iterativeimputer.html#sklearn.impute.iterativeimputer ['../modules/generated/sklearn.impute.iterativeimputer.html#sklearn.impute.iterativeimputer', '../../modules/generated/sklearn.impute.iterativeimputer.html#sklearn.impute.iterativeimputer']
: modules/generated/sklearn.preprocessing.standardscaler.html#sklearn.preprocessing.standardscaler ['../modules/generated/sklearn.preprocessing.standardscaler.html#sklearn.preprocessing.standardscaler', 'modules/generated/sklearn.preprocessing.standardscaler.html#sklearn.preprocessing.standardscaler', '../../modules/generated/sklearn.preprocessing.standardscaler.html#sklearn.preprocessing.standardscaler']

We see there are a lot of cases where two different strings refer to
the same location due to the relative link structure. Relative paths
are something the can be rebuilt automatically, so we'll normalize
these as well:

#+begin_src python
all_links_processed = [norm_relative_link(l) for l in all_links_lower_norm if l != '']
print(len(all_links_set))
print(len(set(all_links_processed)))
#+end_src

#+RESULTS:
: 16916
: 16162

We see that we have reduced the total number of classes by about 800,
but this is likely still too many.

** Reducing the number of classes
We look at the number of examples we have for each link:

#+begin_src python
counter = defaultdict(int)

for link in all_links_processed:
    counter[link] += 1

def count_eq_to(counts, n):
    return [k for k, v in counts.items() if v == n]

freq_links = []
print('Total links:', len(counter))
for n in range(50):
    links = count_eq_to(counter, n)
    c = len(links)
    print(f'Number of links with {n} example(s):', c, sep='\t')
    if n > 25:
        freq_links += links

print('Links with 25 or more examples:', freq_links)
#+end_src

#+RESULTS:
#+begin_example
Total links: 16162
Number of links with 0 example(s):	0
Number of links with 1 example(s):	10870
Number of links with 2 example(s):	3255
Number of links with 3 example(s):	666
Number of links with 4 example(s):	323
Number of links with 5 example(s):	228
Number of links with 6 example(s):	159
Number of links with 7 example(s):	96
Number of links with 8 example(s):	95
Number of links with 9 example(s):	83
Number of links with 10 example(s):	50
Number of links with 11 example(s):	37
Number of links with 12 example(s):	38
Number of links with 13 example(s):	30
Number of links with 14 example(s):	24
Number of links with 15 example(s):	16
Number of links with 16 example(s):	15
Number of links with 17 example(s):	13
Number of links with 18 example(s):	12
Number of links with 19 example(s):	15
Number of links with 20 example(s):	6
Number of links with 21 example(s):	9
Number of links with 22 example(s):	12
Number of links with 23 example(s):	16
Number of links with 24 example(s):	7
Number of links with 25 example(s):	4
Number of links with 26 example(s):	5
Number of links with 27 example(s):	3
Number of links with 28 example(s):	3
Number of links with 29 example(s):	3
Number of links with 30 example(s):	0
Number of links with 31 example(s):	2
Number of links with 32 example(s):	4
Number of links with 33 example(s):	4
Number of links with 34 example(s):	2
Number of links with 35 example(s):	2
Number of links with 36 example(s):	4
Number of links with 37 example(s):	3
Number of links with 38 example(s):	2
Number of links with 39 example(s):	2
Number of links with 40 example(s):	2
Number of links with 41 example(s):	1
Number of links with 42 example(s):	1
Number of links with 43 example(s):	1
Number of links with 44 example(s):	6
Number of links with 45 example(s):	2
Number of links with 46 example(s):	0
Number of links with 47 example(s):	0
Number of links with 48 example(s):	0
Number of links with 49 example(s):	0
Links with 25 or more examples: ['#id10', 'modules/generated/sklearn.ensemble.isolationforest.html#sklearn.ensemble.isolationforest', 'modules/generated/sklearn.tree.decisiontreeregressor.html#sklearn.tree.decisiontreeregressor', 'https://sites.google.com/site/peterprettenhofer/', 'classes.html#module-sklearn.cluster', 'https://github.com/ogrisel', 'glossary.html#term-get_feature_names_out', 'http://fa.bianp.net', 'https://github.com/qinhanmin2014', 'glossary.html#term-cv-splitter', 'classes.html#module-sklearn.metrics.pairwise', '#id7', 'modules/generated/sklearn.tree.decisiontreeclassifier.html#sklearn.tree.decisiontreeclassifier', 'modules/generated/sklearn.preprocessing.onehotencoder.html#sklearn.preprocessing.onehotencoder', 'modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.countvectorizer', 'classes.html#module-sklearn.preprocessing', 'modules/generated/sklearn.linear_model.ridge.html#sklearn.linear_model.ridge', 'http://www.montefiore.ulg.ac.be/~glouppe/', 'glossary.html#term-warm_start', 'classes.html#module-sklearn.model_selection', 'modules/generated/sklearn.manifold.tsne.html#sklearn.manifold.tsne', 'modules/generated/sklearn.preprocessing.standardscaler.html#sklearn.preprocessing.standardscaler', 'modules/generated/sklearn.svm.svc.html#sklearn.svm.svc', 'glossary.html#term-decision_function', 'modules/generated/sklearn.compose.columntransformer.html#sklearn.compose.columntransformer', 'modules/generated/sklearn.ensemble.randomforestclassifier.html#sklearn.ensemble.randomforestclassifier', 'https://github.com/micky774', 'cross_validation.html#cross-validation', 'https://manojbits.wordpress.com', 'http://www.ajoly.org', 'https://github.com/larsmans', 'sklearn.utils.bunch.html#sklearn.utils.bunch', 'https://github.com/adrinjalali', 'glossary.html#term-predict_proba', 'http://www.mblondel.org', 'modules/generated/sklearn.model_selection.randomizedsearchcv.html#sklearn.model_selection.randomizedsearchcv', 'https://github.com/scikit-learn/scikit-learn/blob/449940985/sklearn/base.py#l908', 'modules/generated/sklearn.linear_model.sgdclassifier.html#sklearn.linear_model.sgdclassifier', 'https://github.com/scikit-learn/scikit-learn/blob/449940985/sklearn/base.py#l623', 'https://github.com/lorentzenchr', 'https://github.com/tomdlt', '#id6', 'modules/generated/sklearn.ensemble.gradientboostingclassifier.html#sklearn.ensemble.gradientboostingclassifier', 'modules/generated/sklearn.decomposition.pca.html#sklearn.decomposition.pca', 'modules/generated/sklearn.ensemble.gradientboostingregressor.html#sklearn.ensemble.gradientboostingregressor', 'modules/generated/sklearn.pipeline.pipeline.html#sklearn.pipeline.pipeline', 'https://gael-varoquaux.info', 'modules/generated/sklearn.ensemble.histgradientboostingclassifier.html#sklearn.ensemble.histgradientboostingclassifier', 'http://alexandre.gramfort.net', 'classes.html#module-sklearn.linear_model', '#id5', 'classes.html#module-sklearn.datasets']
#+end_example

We see that the majority of links have few examples, and a minority
are heavily used. Fortunately for this application, we can focus our
efforts on classifying the most heavily used links, as those are the
links which will most likely be needed again.

We can look at the number of classification classes depending on our
minimum threshold for the number of available examples:

#+begin_src python
def count_ge_to(counts, n):
    return [k for k, v in counts.items() if v >= n]

for n in range(50):
    links = count_ge_to(counter, n)
    print(f'Number of classes with at least {n} example(s):', len(links), sep='\t')
#+end_src

#+RESULTS:
#+begin_example
Number of classes with at least 0 example(s):	16162
Number of classes with at least 1 example(s):	16162
Number of classes with at least 2 example(s):	5292
Number of classes with at least 3 example(s):	2037
Number of classes with at least 4 example(s):	1371
Number of classes with at least 5 example(s):	1048
Number of classes with at least 6 example(s):	820
Number of classes with at least 7 example(s):	661
Number of classes with at least 8 example(s):	565
Number of classes with at least 9 example(s):	470
Number of classes with at least 10 example(s):	387
Number of classes with at least 11 example(s):	337
Number of classes with at least 12 example(s):	300
Number of classes with at least 13 example(s):	262
Number of classes with at least 14 example(s):	232
Number of classes with at least 15 example(s):	208
Number of classes with at least 16 example(s):	192
Number of classes with at least 17 example(s):	177
Number of classes with at least 18 example(s):	164
Number of classes with at least 19 example(s):	152
Number of classes with at least 20 example(s):	137
Number of classes with at least 21 example(s):	131
Number of classes with at least 22 example(s):	122
Number of classes with at least 23 example(s):	110
Number of classes with at least 24 example(s):	94
Number of classes with at least 25 example(s):	87
Number of classes with at least 26 example(s):	83
Number of classes with at least 27 example(s):	78
Number of classes with at least 28 example(s):	75
Number of classes with at least 29 example(s):	72
Number of classes with at least 30 example(s):	69
Number of classes with at least 31 example(s):	69
Number of classes with at least 32 example(s):	67
Number of classes with at least 33 example(s):	63
Number of classes with at least 34 example(s):	59
Number of classes with at least 35 example(s):	57
Number of classes with at least 36 example(s):	55
Number of classes with at least 37 example(s):	51
Number of classes with at least 38 example(s):	48
Number of classes with at least 39 example(s):	46
Number of classes with at least 40 example(s):	44
Number of classes with at least 41 example(s):	42
Number of classes with at least 42 example(s):	41
Number of classes with at least 43 example(s):	40
Number of classes with at least 44 example(s):	39
Number of classes with at least 45 example(s):	33
Number of classes with at least 46 example(s):	31
Number of classes with at least 47 example(s):	31
Number of classes with at least 48 example(s):	31
Number of classes with at least 49 example(s):	31
#+end_example

We can visualize the relationship between the number of examples and
the number of classes:

#+BEGIN_src python :results output file :file numclasses.png :output-dir images/
import sys
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
x = range(50)
ax.plot(x, [len(count_ge_to(counter, n)) for n in x])
ax.set_xlabel('Num examples')
ax.set_ylabel('Num classes')
plt.ylim([0, 800])
fig.tight_layout()

plt.savefig(sys.stdout.buffer)
#+END_src

#+RESULTS:
[[file:images/numclasses.png]]

* Train/test splitting
We can make preprocessed data splits using:

#+begin_src shell :eval no
python train_test_split.py data/html/processed/ <min_examples> <test_size>
#+end_src

This script is a tool to split the data while trying to keep a good
amount of examples in the training and test sets, but it's not
perfect.

We'll explore which data set provides the best balance between number
of classes and number of examples in the test and train sets.

#+begin_src python
def show_data_summary(paths, desired_examples, show_classes):
    docs_data = []
    for p in paths:
        with open(p, 'r') as f:
            link_data = json.load(f)
            docs_data.append(link_data)
    totals = defaultdict(int)
    for v in (v for d in docs_data for v in d.values()):
        link = v['link']
        if link is not None:
            totals[link] += 1
    print('Total classes:\t', len(totals))
    print('Total classes with desired number of examples:\t', len({k: v for k, v in totals.items() if v >= desired_examples}))
    if show_classes:
        print('Classes:', sorted([(v, k) for k, v in totals.items()], reverse=True))
    return len(totals)
#+end_src

#+RESULTS:

#+begin_src python
def summarize_data_split(min_examples, test_size, show_classes=False):
    pattern = os.path.join('split', f'train-{min_examples}-{test_size*100:.0f}', '**', '*' + '.linkdata.json')
    trainpaths = glob.glob(pattern, recursive=True)
    pattern = os.path.join('split', f'test-{min_examples}-{test_size*100:.0f}', '**', '*' + '.linkdata.json')
    testpaths = glob.glob(pattern, recursive=True)
    print('Min examples:', min_examples, '| Test size:', test_size, f'| Train/test counts: {len(trainpaths)}/{len(testpaths)}')
    print('(train)')
    trainclasses = show_data_summary(trainpaths, 2, show_classes)
    print('(test)')
    testclasses = show_data_summary(testpaths, 2, show_classes)
    print('Num classes with no test examples:', trainclasses - testclasses)
    print()
#+end_src

#+RESULTS:

#+begin_src python
summarize_data_split(5, 0.05)
summarize_data_split(10, 0.05)
summarize_data_split(15, 0.05)
summarize_data_split(20, 0.05, show_classes=False)
summarize_data_split(25, 0.05)
summarize_data_split(30, 0.05)
#+end_src

#+RESULTS:
#+begin_example
Min examples: 5 | Test size: 0.05 | Train/test counts: 947/49
(train)
Total classes:	 1048
Total classes with desired number of examples:	 1047
(test)
Total classes:	 319
Total classes with desired number of examples:	 152
Num classes with no test examples: 729

Min examples: 10 | Test size: 0.05 | Train/test counts: 947/49
(train)
Total classes:	 387
Total classes with desired number of examples:	 387
(test)
Total classes:	 230
Total classes with desired number of examples:	 127
Num classes with no test examples: 157

Min examples: 15 | Test size: 0.05 | Train/test counts: 947/49
(train)
Total classes:	 208
Total classes with desired number of examples:	 208
(test)
Total classes:	 171
Total classes with desired number of examples:	 106
Num classes with no test examples: 37

Min examples: 20 | Test size: 0.05 | Train/test counts: 947/49
(train)
Total classes:	 137
Total classes with desired number of examples:	 137
(test)
Total classes:	 136
Total classes with desired number of examples:	 86
Num classes with no test examples: 1

Min examples: 25 | Test size: 0.05 | Train/test counts: 947/49
(train)
Total classes:	 87
Total classes with desired number of examples:	 87
(test)
Total classes:	 86
Total classes with desired number of examples:	 70
Num classes with no test examples: 1

Min examples: 30 | Test size: 0.05 | Train/test counts: 947/49
(train)
Total classes:	 69
Total classes with desired number of examples:	 69
(test)
Total classes:	 69
Total classes with desired number of examples:	 59
Num classes with no test examples: 0
#+end_example

#+begin_src python
summarize_data_split(5, 0.07)
summarize_data_split(10, 0.07)
summarize_data_split(15, 0.07)
summarize_data_split(20, 0.07, show_classes=False)
summarize_data_split(25, 0.07)
summarize_data_split(30, 0.07)
#+end_src

#+RESULTS:
#+begin_example
Min examples: 5 | Test size: 0.07 | Train/test counts: 927/69
(train)
Total classes:	 1048
Total classes with desired number of examples:	 1045
(test)
Total classes:	 447
Total classes with desired number of examples:	 227
Num classes with no test examples: 601

Min examples: 10 | Test size: 0.07 | Train/test counts: 927/69
(train)
Total classes:	 387
Total classes with desired number of examples:	 387
(test)
Total classes:	 303
Total classes with desired number of examples:	 208
Num classes with no test examples: 84

Min examples: 15 | Test size: 0.07 | Train/test counts: 927/69
(train)
Total classes:	 208
Total classes with desired number of examples:	 208
(test)
Total classes:	 207
Total classes with desired number of examples:	 147
Num classes with no test examples: 1

Min examples: 20 | Test size: 0.07 | Train/test counts: 927/69
(train)
Total classes:	 137
Total classes with desired number of examples:	 137
(test)
Total classes:	 136
Total classes with desired number of examples:	 113
Num classes with no test examples: 1

Min examples: 25 | Test size: 0.07 | Train/test counts: 927/69
(train)
Total classes:	 87
Total classes with desired number of examples:	 87
(test)
Total classes:	 86
Total classes with desired number of examples:	 83
Num classes with no test examples: 1

Min examples: 30 | Test size: 0.07 | Train/test counts: 927/69
(train)
Total classes:	 69
Total classes with desired number of examples:	 69
(test)
Total classes:	 69
Total classes with desired number of examples:	 69
Num classes with no test examples: 0
#+end_example

#+begin_src python
summarize_data_split(5, 0.15)
summarize_data_split(10, 0.15)
summarize_data_split(15, 0.15)
summarize_data_split(20, 0.15, show_classes=False)
summarize_data_split(25, 0.15)
summarize_data_split(30, 0.15)
#+end_src

#+RESULTS:
#+begin_example
Min examples: 5 | Test size: 0.15 | Train/test counts: 847/149
(train)
Total classes:	 1048
Total classes with desired number of examples:	 1003
(test)
Total classes:	 812
Total classes with desired number of examples:	 533
Num classes with no test examples: 236

Min examples: 10 | Test size: 0.15 | Train/test counts: 847/149
(train)
Total classes:	 387
Total classes with desired number of examples:	 376
(test)
Total classes:	 380
Total classes with desired number of examples:	 326
Num classes with no test examples: 7

Min examples: 15 | Test size: 0.15 | Train/test counts: 847/149
(train)
Total classes:	 208
Total classes with desired number of examples:	 208
(test)
Total classes:	 207
Total classes with desired number of examples:	 207
Num classes with no test examples: 1

Min examples: 20 | Test size: 0.15 | Train/test counts: 847/149
(train)
Total classes:	 137
Total classes with desired number of examples:	 137
(test)
Total classes:	 136
Total classes with desired number of examples:	 136
Num classes with no test examples: 1

Min examples: 25 | Test size: 0.15 | Train/test counts: 847/149
(train)
Total classes:	 87
Total classes with desired number of examples:	 87
(test)
Total classes:	 86
Total classes with desired number of examples:	 86
Num classes with no test examples: 1

Min examples: 30 | Test size: 0.15 | Train/test counts: 847/149
(train)
Total classes:	 69
Total classes with desired number of examples:	 69
(test)
Total classes:	 69
Total classes with desired number of examples:	 69
Num classes with no test examples: 0
#+end_example

#+begin_src python
summarize_data_split(5, 0.33)
summarize_data_split(10, 0.33)
summarize_data_split(15, 0.33)
summarize_data_split(20, 0.33, show_classes=False)
summarize_data_split(25, 0.33)
summarize_data_split(30, 0.33)
#+end_src

#+RESULTS:
#+begin_example
Min examples: 5 | Test size: 0.33 | Train/test counts: 668/328
(train)
Total classes:	 1044
Total classes with desired number of examples:	 961
(test)
Total classes:	 948
Total classes with desired number of examples:	 807
Num classes with no test examples: 96

Min examples: 10 | Test size: 0.33 | Train/test counts: 668/328
(train)
Total classes:	 387
Total classes with desired number of examples:	 374
(test)
Total classes:	 379
Total classes with desired number of examples:	 378
Num classes with no test examples: 8

Min examples: 15 | Test size: 0.33 | Train/test counts: 668/328
(train)
Total classes:	 208
Total classes with desired number of examples:	 208
(test)
Total classes:	 207
Total classes with desired number of examples:	 207
Num classes with no test examples: 1

Min examples: 20 | Test size: 0.33 | Train/test counts: 668/328
(train)
Total classes:	 137
Total classes with desired number of examples:	 137
(test)
Total classes:	 136
Total classes with desired number of examples:	 136
Num classes with no test examples: 1

Min examples: 25 | Test size: 0.33 | Train/test counts: 668/328
(train)
Total classes:	 87
Total classes with desired number of examples:	 87
(test)
Total classes:	 86
Total classes with desired number of examples:	 86
Num classes with no test examples: 1

Min examples: 30 | Test size: 0.33 | Train/test counts: 668/328
(train)
Total classes:	 69
Total classes with desired number of examples:	 69
(test)
Total classes:	 69
Total classes with desired number of examples:	 69
Num classes with no test examples: 0
#+end_example

* Removing duplicates
#+begin_src python
test_db = DocBin().from_disk('data/iter6/test.spacy')
train_db = DocBin().from_disk('data/iter6/train.spacy')
test_docs = list(test_db.get_docs(trained.vocab))
train_docs = list(train_db.get_docs(trained.vocab))
print(len(test_docs), len(train_docs))
#+end_src

#+RESULTS:
: 386 3860

#+begin_src python
def remove_duplicates(docs, seen):
    new_docs = []
    for i, doc in enumerate(docs):
        # print('Doc:', i)
        spans = doc.spans['sc']
        spans_set = set(spans)
        spans_used = set()
        no_duplicates = True
        for tokens in sliding_window(doc, 8):
            ts = (' '.join(t.text.strip() for t in tokens)).lower()
            all_spans_used = False
            for span in filter(lambda s: any(t in s for t in tokens), spans):
                if (ts, span.label) in seen:
                    # print('Duplicate found:', ts)
                    no_duplicates = False
                    all_spans_used = spans_used == spans_set
                else:
                    seen.add((ts, span.label))
                    spans_used.add(span)
            if all_spans_used:
                break
        if no_duplicates:
            new_docs.append(doc)
    return new_docs, seen
#+end_src

#+RESULTS:

#+begin_src python
seen = set()
new_test_docs, seen = remove_duplicates(test_docs, seen)
print(len(test_docs), len(new_test_docs))
#+end_src

#+RESULTS:
: 386 259

#+begin_src python
new_train_docs, seen = remove_duplicates(train_docs, seen)
print(len(train_docs), len(new_train_docs))
#+end_src

#+RESULTS:
: 3860 1341

#+begin_src python
def show_docs_summary(docs, desired_examples, show_classes):
    totals = defaultdict(int)
    for link in (s.label_ for d in docs for s in d.spans['sc']):
        totals[link] += 1
    print('Total classes:\t', len(totals))
    print('Total classes with desired number of examples:\t', len({k: v for k, v in totals.items() if v >= desired_examples}))
    if show_classes:
        print('Classes:', sorted([(v, k) for k, v in totals.items()], reverse=True))
    return len(totals)
#+end_src

#+RESULTS:

#+begin_src python
def summarize_docs_split(train_docs, test_docs, show_classes=False):
    print(f'Train/test counts: {len(train_docs)}/{len(test_docs)}')
    print('(train)')
    trainclasses = show_docs_summary(train_docs, 2, show_classes)
    print('(test)')
    testclasses = show_docs_summary(test_docs, 1, show_classes)
    print('Num classes with no test examples:', trainclasses - testclasses)
    print()
#+end_src

#+RESULTS:

#+begin_src python
summarize_data_split(30, 0.05)
print()
summarize_docs_split(new_train_docs, new_test_docs)
summarize_docs_split(train_docs, test_docs)
#+end_src

#+RESULTS:
#+begin_example
Min examples: 30 | Test size: 0.05 | Train/test counts: 947/49
(train)
Total classes:	 69
Total classes with desired number of examples:	 69
(test)
Total classes:	 69
Total classes with desired number of examples:	 59
Num classes with no test examples: 0


Train/test counts: 1341/259
(train)
Total classes:	 60
Total classes with desired number of examples:	 59
(test)
Total classes:	 63
Total classes with desired number of examples:	 63
Num classes with no test examples: -3

Train/test counts: 3860/386
(train)
Total classes:	 69
Total classes with desired number of examples:	 69
(test)
Total classes:	 63
Total classes with desired number of examples:	 63
Num classes with no test examples: 6
#+end_example

#+begin_src python
def make_db(docs, name):
    db = DocBin()
    for doc in docs:
        db.add(doc)
    db.to_disk(f'{name}.spacy')
#+end_src

#+RESULTS:

#+begin_src python
make_db(new_train_docs, 'train-no-dups')
make_db(new_test_docs, 'test-no-dups')
#+end_src

#+RESULTS:
